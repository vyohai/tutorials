{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5087095",
   "metadata": {},
   "source": [
    "# pytorch on CPU performance\n",
    " deals with running time of pytorch models on different pc, and performence issues.\n",
    " \n",
    " 1. CPU optimization with MKL/OpenBLAS - config for different CPUs\n",
    " 2. Number of CPU Threads - how many threads and their effect\n",
    " 3. CPU thermal issues - temperature issues\n",
    " 4. Background Load - CPU and memory ussage and their effect - TBD\n",
    " 5. Storage performance - type of memoty access and disk speed - TBD\n",
    " 6. Memory Bandwidth/NUMA effects - ?- TBD\n",
    " 7. python and pytorch version - TBD\n",
    " 8. model specipic issues - TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e9b7ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddcfd8b",
   "metadata": {},
   "source": [
    "## CPU optimization with MKL/OpenBLAS - config for different CPUs\n",
    "\n",
    "* is  pytorch using modern instructions(VX2\\AVX512) and not SSE\n",
    "\n",
    "* does Thermal Throteling prevents good use of heavy instructions?TBD.....................\n",
    "\n",
    "* does the worksattion is faster for a single(or 2) threads? i can check by setting the thread number\n",
    "\n",
    "* does the MKL\\OpenBlas deafault to using a lot of threads= you could check the env var like  \"MKL_NUM_THREADS\", they could even if you a single thread loop in pytorch\n",
    "\n",
    "* does using alot of threads is bad?= can cause too much overhead(simple matrix multiplication runs best on 4 threads and slow on 32, so a cpu with less cores could default to a more afficient number of threads\n",
    "\n",
    "* env variables which may force bad threading(OMP_NUM_THREADS\\MKL_NUM_THREADS\\NUMEXPR_MAX_THREADS\\MKL_DEBUG_CPU_TYPE), what each does?\n",
    "\n",
    "### Concepts\n",
    " **MKL** -  library for math and matrix multiplication optimization\n",
    " \n",
    " **OpenMP** - library for multithreading\n",
    " \n",
    " **NUMA_aware** -\n",
    " \n",
    " **CPU instructions** - (AVX\\AVX2\\AVX-512\\SSE...), in modern cpu should be at least AVX2\n",
    " \n",
    " **Threads scheduler** - \n",
    " \n",
    " **Number of Threads** - too much threads could do too much overhead\n",
    " \n",
    " **OpenBLAS** - open source MKL like library probably slower\n",
    " \n",
    " **Eigen** - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17f74878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch built with:\n",
      "  - C++ Version: 199711\n",
      "  - MSVC 192829337\n",
      "  - Intel(R) Math Kernel Library Version 2020.0.2 Product Build 20200624 for Intel(R) 64 architecture applications\n",
      "  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)\n",
      "  - OpenMP 2019\n",
      "  - LAPACK is enabled (usually provided by MKL)\n",
      "  - CPU capability usage: AVX2\n",
      "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CXX_COMPILER=C:/actions-runner/_work/pytorch/pytorch/builder/windows/tmp_bin/sccache-cl.exe, CXX_FLAGS=/DWIN32 /D_WINDOWS /GR /EHsc /w /bigobj -DUSE_PTHREADPOOL -openmp:experimental -IC:/actions-runner/_work/pytorch/pytorch/builder/windows/mkl/include -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOCUPTI -DUSE_FBGEMM -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.1, USE_CUDA=0, USE_CUDNN=OFF, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=OFF, USE_NNPACK=OFF, USE_OPENMP=ON, USE_ROCM=OFF, \n",
      "\n",
      "\n",
      "env variables\n",
      "\t {'COMPUTERNAME': 'DESKTOP-2VT5SGT', 'NUMBER_OF_PROCESSORS': '4', 'PROMPT': '$P$G'}\n",
      "None\n",
      "\n",
      "number of threads:\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(torch.__config__.show())\n",
    "\n",
    "print(\"\\nenv variables\")\n",
    "print(\"\\t\",{k: v for k, v in os.environ.items() if \"OMP\" in k or \"MKL\" in k or \"NUM\" in k})\n",
    "\n",
    "print(os.environ.get(\"MKL_NUM_THREADS\"))\n",
    "\n",
    "print(\"\\nnumber of threads:\")\n",
    "print(torch.get_num_threads())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d23ba4",
   "metadata": {},
   "source": [
    "in this case :\n",
    "\n",
    "  - Intel(R) Math Kernel Library Version 2020.0.2 Product Build 20200624 for Intel(R) 64 architecture applications\n",
    "  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)\n",
    "  - OpenMP 2019\n",
    "  - LAPACK is enabled (usually provided by MKL)\n",
    "  - CPU capability usage: AVX2\n",
    "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CXX_COMPILER=C:/actions-runner/_work/pytorch/pytorch/builder/windows/tmp_bin/sccache-cl.exe, CXX_FLAGS=/DWIN32 /D_WINDOWS /GR /EHsc /w /bigobj -DUSE_PTHREADPOOL -openmp:experimental -IC:/actions-runner/_work/pytorch/pytorch/builder/windows/mkl/include -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOCUPTI -DUSE_FBGEMM -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.1, USE_CUDA=0, USE_CUDNN=OFF, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=OFF, USE_NNPACK=OFF, USE_OPENMP=ON, USE_ROCM=OFF, \n",
    "  \n",
    " * your using MKL version 2020.0.2(for linear algebra)\n",
    " \n",
    " * and MKL-DNN v2.6.0(for deep learning apps)\n",
    " * and OPENMP(for multithreading)\n",
    " * and LAPACK(for matrix solvers)\n",
    " \n",
    " * CPU uses VX2\n",
    " \n",
    " * BLAS using MKL\n",
    " * both MKL and MKL-DNN and OpenMP are on and enabled\n",
    " * CUDA and CUDNN are off and disabled\n",
    " \n",
    " * all AVX(2\\512) are enablesd and the use is on AVX2 because the cpu probably dont support AVX512\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531b0147",
   "metadata": {},
   "source": [
    "### to control number of threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e76ab93e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(os.environ.get(\"MKL_NUM_THREADS\"))\n",
    "#os.environ[\"MKL_NUM_THREADS\"]=4\n",
    "# and\n",
    "#torch.set_num_threads(4)\n",
    "# and\n",
    "#os.environ[\"OMP_NUM_THREADS\"]=4\n",
    "# now the script is using 4 threads\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e236d4e8",
   "metadata": {},
   "source": [
    "## CPU thermal issues\n",
    "the speed under full load should be around the base clock value if its below there is some throtelling, you can tell clocks values by the cpu model \n",
    "### concepts\n",
    "\n",
    "**Base clock** - minimum guretedd speed under load without over heating\n",
    "**Boost clock** - higher clock can be reach temprarly, ussually when the load starts it boost and then goes to base speed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
